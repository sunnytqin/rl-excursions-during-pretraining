<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link
      rel="preload"
      href="../fonts/ibm_plex_sans/IBMPlexSans-Regular.ttf"
      as="font"
      type="font/ttf"
      crossorigin
    />
    <link
      rel="preload"
      href="../fonts/newsreader/Newsreader_14pt-Italic.ttf"
      as="font"
      type="font/ttf"
      crossorigin
    />
    <!-- Favicon: replace with your own when ready -->
    <link rel="icon" type="image/jpeg" href="../assets/figures/isocompute_ico.jpg" />
    <!-- Keep .ico as a fallback for older clients -->
    <!-- <link rel="icon" href="../favicon.ico" /> -->
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-ZSV2YDNY0W"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "G-ZSV2YDNY0W", { send_page_view: false });
    </script>
    <script>
      if (window.location.hostname === "localhost") {
        window.gtag = () => {};
      }
    </script>
    
		<link href="../_app/immutable/assets/0.ChBBf1Xn.css" rel="stylesheet">
		<link href="../_app/immutable/assets/rl_excursions.CZR1otz5.css" rel="stylesheet">
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_AMS-Regular.BQhdFMY1.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_AMS-Regular.DMm9YOAa.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_AMS-Regular.DRggAlZN.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Caligraphic-Bold.Dq_IR9rO.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Caligraphic-Bold.BEiXGLvX.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Caligraphic-Bold.ATXxdsX0.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Caligraphic-Regular.Di6jR-x-.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Caligraphic-Regular.CTRA-rTL.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Caligraphic-Regular.wX97UBjC.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Fraktur-Bold.CL6g_b3V.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Fraktur-Bold.BsDP51OF.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Fraktur-Bold.BdnERNNW.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Fraktur-Regular.CTYiF6lA.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Fraktur-Regular.Dxdc4cR9.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Fraktur-Regular.CB_wures.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Main-Bold.Cx986IdX.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Main-Bold.Jm3AIy58.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Main-Bold.waoOVXN0.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Main-BoldItalic.DxDJ3AOS.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Main-BoldItalic.SpSLRI95.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Main-BoldItalic.DzxPMmG6.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Main-Italic.NWA7e6Wa.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Main-Italic.BMLOBm91.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Main-Italic.3WenGoN9.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Main-Regular.B22Nviop.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Main-Regular.Dr94JaBh.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Main-Regular.ypZvNtVU.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Math-BoldItalic.CZnvNsCZ.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Math-BoldItalic.iY-2wyZ7.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Math-BoldItalic.B3XSjfu4.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Math-Italic.t53AETM-.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Math-Italic.DA0__PXp.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Math-Italic.flOr_0UB.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_SansSerif-Bold.D1sUS0GD.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_SansSerif-Bold.DbIhKOiC.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_SansSerif-Bold.CFMepnvq.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_SansSerif-Italic.C3H0VqGB.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_SansSerif-Italic.DN2j7dab.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_SansSerif-Italic.YYjJ1zSn.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_SansSerif-Regular.DDBCnlJ7.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_SansSerif-Regular.CS6fqUqJ.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_SansSerif-Regular.BNo7hRIc.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Script-Regular.D3wIWfF6.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Script-Regular.D5yQViql.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Script-Regular.C5JkGWo-.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Size1-Regular.mCD8mA8B.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Size1-Regular.C195tn64.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Size1-Regular.Dbsnue_I.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Size2-Regular.Dy4dx90m.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Size2-Regular.oD1tc_U0.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Size2-Regular.B7gKUWhC.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Size3-Regular.CTq5MqoE.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Size3-Regular.DgpXs0kz.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Size4-Regular.Dl5lxZxV.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Size4-Regular.BF-4gkZK.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Size4-Regular.DWFBv043.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Typewriter-Regular.CO6r4hn1.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Typewriter-Regular.C0xS9mPB.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Typewriter-Regular.D3Ib7_Hf.ttf" crossorigin>
		<link rel="modulepreload" href="../_app/immutable/entry/start.Dz3fLiJ5.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/entry.BY9VUEtR.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/scheduler.CHFMnfDQ.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/paths.C95So0OC.js">
		<link rel="modulepreload" href="../_app/immutable/entry/app.nvD-W9l8.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/index.BvnlO6mU.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/0.Qd4HBCtq.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/each.Cn-hsyDx.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/stores.DukBHylc.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/3.D37lg1JT.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/rl_excursions.Dhysu1M1.js"><title>RL Excursions during Pre-training</title><!-- HEAD_svelte-yrkxwy_START --><script async src="https://www.googletagmanager.com/gtag/js?id=G-Z1XRQ6ZG3X" data-svelte-h="svelte-1mc97wv"></script><script data-svelte-h="svelte-woggic">window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag("js", new Date());
    gtag("config", "G-Z1XRQ6ZG3X", { send_page_view: false });
  </script><!-- HEAD_svelte-yrkxwy_END --><!-- HEAD_svelte-1u7ey3_START --><meta name="description" content="RL Excursions during Pre-training: How early is too early for On-policy Learning?"><meta property="og:title" content="RL Excursions during Pre-training"><meta property="og:description" content="RL Excursions during Pre-training: How early is too early for On-policy Learning?"><!-- HEAD_svelte-1u7ey3_END -->
  </head>
  <body data-sveltekit-preload-data="hover">
    <div style="display: contents">   <div class="page-upper-right-inline svelte-pbnrg0" data-svelte-h="svelte-6nno4e"><img src="../assets/figures/upper_right_final.png" alt="Institution logos" class="page-upper-right svelte-pbnrg0" loading="lazy" decoding="async"></div> <header class="layout-xl justify-between items-start" data-sveltekit-noscroll data-sveltekit-preload-code="eager"><div class="header-inner mb-8 svelte-1j41yyi"><h1 class="title-font font-bold text-black text-3xl mb-4 leading-tight svelte-1j41yyi">RL Excursions during Pre-training: How early is too early for On-policy Learning?</h1> <div class="meta text-black svelte-1j41yyi"><div class="authors svelte-1j41yyi"><span class="author svelte-1j41yyi"><span class="author-name svelte-1j41yyi">Rachit Bansal</span> <sup class="affil-sup svelte-1j41yyi"><img src="../assets/figures/harvard.svg" alt="Harvard University" title="Harvard University" class="affil-logo   affil-logo--harvard svelte-1j41yyi" loading="lazy" decoding="async"> <span class="affil-sup-text svelte-1j41yyi" data-svelte-h="svelte-17ywuv4">*</span> </sup> </span><span class="author svelte-1j41yyi"><span class="author-name svelte-1j41yyi">Tian Qin</span> <sup class="affil-sup svelte-1j41yyi"><img src="../assets/figures/harvard.svg" alt="Harvard University" title="Harvard University" class="affil-logo   affil-logo--harvard svelte-1j41yyi" loading="lazy" decoding="async"> <span class="affil-sup-text svelte-1j41yyi" data-svelte-h="svelte-17ywuv4">*</span> </sup> </span><span class="author svelte-1j41yyi"><span class="author-name svelte-1j41yyi">Clara Mohri</span> <sup class="affil-sup svelte-1j41yyi"><img src="../assets/figures/harvard.svg" alt="Harvard University" title="Harvard University" class="affil-logo   affil-logo--harvard svelte-1j41yyi" loading="lazy" decoding="async"> <span class="affil-sup-text svelte-1j41yyi" data-svelte-h="svelte-17ywuv4">*</span> </sup> </span><span class="author svelte-1j41yyi"><span class="author-name svelte-1j41yyi">David Alvarez-Melis</span> <sup class="affil-sup svelte-1j41yyi"><img src="../assets/figures/harvard.svg" alt="Harvard University" title="Harvard University" class="affil-logo   affil-logo--harvard svelte-1j41yyi" loading="lazy" decoding="async">  </sup> </span><span class="author svelte-1j41yyi"><span class="author-name svelte-1j41yyi">Sham Kakade</span> <sup class="affil-sup svelte-1j41yyi"><img src="../assets/figures/harvard.svg" alt="Harvard University" title="Harvard University" class="affil-logo   affil-logo--harvard svelte-1j41yyi" loading="lazy" decoding="async">  </sup> </span></div> <div class="affiliations svelte-1j41yyi"><div class="affiliation-line svelte-1j41yyi"><span class="affiliation-item svelte-1j41yyi"><img src="../assets/figures/harvard.svg" alt="Harvard University" title="Harvard University" class="affil-logo affil-logo--legend svelte-1j41yyi" loading="lazy" decoding="async"> <span>Harvard University</span></span> <span class="affiliation-sep svelte-1j41yyi" data-svelte-h="svelte-djw72">, </span><span class="affiliation-item svelte-1j41yyi"> <span>* Equal contribution</span></span> </div></div> <div class="date svelte-1j41yyi"> </div></div></div> </header> <main>   <div> <nav class="toc svelte-wsnayn" aria-hidden="true"> </nav> <div class="layout-xl text-base space-y-12"><div class="md-shell svelte-1cybysc"><div class="md-grid svelte-1cybysc"><div class="md-output space-y-6 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><!-- ---
title: "RL Excursions During Pre-Training: How Early Is Too Early for On-Policy Learning?"
subtitle: "When can a language model start learning from its *own* generations?"
authors:
  - "Anonymous authors"
date: "2026-02-11"
tags: [llm-training, reinforcement-learning, reasoning, grpo, rlvr, pretraining]
--- -->

<!--
This post is a blog-style adaptation of the paper:
"RL Excursions During Pre-Training: How Early Is Too Early for On-Policy Learning?"
Replace placeholder figure links in `assets/` with real images exported from the paper.
-->

<!-- # RL excursions during pre-training: how early is *too* early for on-policy learning? -->

<figure>
  <img src="/rl-excursions-during-pretraining/assets/figures/figure_1.png" alt="Overview figure placeholder showing early RL works, expansion vs sharpening, and rollout budget tradeoffs." width="100%"/>
  <figcaption><strong>Figure 1 (placeholder).</strong> A one-picture summary: RL works surprisingly early; RL can <em>expand</em> or <em>sharpen</em> the output distribution depending on the pipeline; rollout count trades off sample-efficiency vs FLOP-efficiency.</figcaption>
</figure>

<p>Modern LLM training usually looks like this:</p>
<blockquote class="inline-block bg-neutral-50 border-l-4 border-neutral-600 rounded px-3 py-2 align-middle my-2"><p><strong>Pretrain (next-token prediction)</strong> → <strong>SFT (next-token prediction)</strong> → <strong>RL (on-policy)</strong></p>
</blockquote><p>This separation raises a simple question that we don’t often test directly:</p>
<p><strong>When does a model become capable of learning from its own generations?</strong></p>
<p>In this post we run a controlled case study on math reasoning, where rewards are unambiguous, and ask:</p>
<blockquote class="inline-block bg-neutral-50 border-l-4 border-neutral-600 rounded px-3 py-2 align-middle my-2"><p><strong>How and when should an RL objective be used in LLM training?</strong></p>
</blockquote><hr>
<!-- HTML_TAG_END --></div><details class="foldbox foldbox--h2 svelte-1cybysc" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h2 id="tl-dr">TL;DR</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><ul>
<li><strong>RL can work <em>much earlier</em> than standard practice.</strong> On GSM8K-style math, running RL directly on intermediate pretraining checkpoints boosts accuracy even when the model is still “under-trained” by standard scaling-law heuristics.</li>
<li><strong>RL-only can match the “gold standard” SFT→RL pipeline (sometimes).</strong> On GSM8K, direct RL on the base checkpoint reaches performance comparable to SFT→RL once the model has seen enough pretraining tokens.</li>
<li><strong>RL doesn’t always just “sharpen.”</strong> In our setting, RL directly on the base model tends to improve both pass@1 <em>and</em> pass@k (distribution <strong>expansion</strong>), while RL after SFT often improves pass@1 but hurts pass@k (distribution <strong>sharpening</strong>).</li>
<li><strong>More rollouts aren’t always better.</strong> Increasing GRPO rollouts per prompt improves <em>sample efficiency</em> but can be <em>worse per FLOP</em>, especially when rewards are sparse.</li>
</ul>
<p>If you only remember one thing: <strong>“Post-training” might not need to wait until <em>after</em> pretraining.</strong></p>
<hr>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h2 svelte-1cybysc" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h2 id="table-of-contents">Table of contents</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><ul>
<li><a href="#the-puzzle-off-policy-vs-on-policy-training" class="link">The puzzle: off-policy vs on-policy training</a></li>
<li><a href="#experimental-setup-in-one-page" class="link">Experimental setup in one page</a></li>
<li><a href="#result-1-rl-is-effective-early-in-pretraining" class="link">Result 1: RL is effective early in pretraining</a></li>
<li><a href="#result-2-rl-can-expand-or-sharpen-the-output-distribution" class="link">Result 2: RL can expand or sharpen the output distribution</a></li>
<li><a href="#result-3-rollout-budgets-under-sparse-rewards" class="link">Result 3: rollout budgets under sparse rewards</a></li>
<li><a href="#practical-takeaways" class="link">Practical takeaways</a></li>
<li><a href="#limitations-and-open-questions" class="link">Limitations and open questions</a></li>
<li><a href="#citation" class="link">Citation</a></li>
<li><a href="#appendix-additional-training-curves-and-ablations" class="link">Appendix: additional training curves and ablations</a></li>
</ul>
<hr>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h2 svelte-1cybysc" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h2 id="the-puzzle-off-policy-vs-on-policy-training">The puzzle: off-policy vs on-policy training</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>Pretraining and supervised fine-tuning (SFT) optimize the <strong>next-token prediction</strong> objective. Conceptually, that’s an <em>off-policy</em> regime: the model is trained to imitate a fixed dataset.</p>
<p>Reinforcement learning (RL), in contrast, is <strong>on-policy</strong>: the model samples outputs from itself, gets feedback, and updates to increase reward.</p>
<p>So the transition from SFT to RL is a big change:</p>
<ul>
<li>from <em>imitation</em> → to <em>self-generated data</em></li>
<li>from <em>dense token-level supervision</em> → to <em>sparse outcome-level supervision</em> (at least in the RLVR setting we study)</li>
</ul>
<p>That makes the following question natural:</p>
<blockquote class="inline-block bg-neutral-50 border-l-4 border-neutral-600 rounded px-3 py-2 align-middle my-2"><p><strong>At what point during pretraining does the model’s self-generated data become good enough that on-policy learning helps rather than hurts?</strong></p>
</blockquote><p>Math is a clean testbed because the reward can be <strong>verifiable</strong> (did the final answer match?).</p>
<hr>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h2 svelte-1cybysc" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h2 id="experimental-setup-in-one-page">Experimental setup in one page</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><!-- HTML_TAG_END --></div><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="what-we-trained">What we trained</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>We pretrain a <strong>1B-parameter</strong> decoder-only model (OLMo2 architecture) from scratch on <strong>50B tokens</strong> of a high-quality mixture (DOLMino, from OLMo2), saving intermediate checkpoints throughout. We then take checkpoints and run different “post-training” pipelines <em>from each checkpoint</em>.</p>
<details>
<summary><strong>Pretraining details (click to expand)</strong></summary>

<ul>
<li><strong>Architecture:</strong> OLMo2 1B</li>
<li><strong>Tokens:</strong> 50B total (≈ 2.5× Chinchilla-optimal token count for this model size)</li>
<li><strong>Optimizer:</strong> AdamW with cosine LR decay, peak LR 4e-4</li>
<li><strong>Seq length:</strong> 4096</li>
<li><strong>Batch size:</strong> 512</li>
<li><strong>Data mixture (DOLMino high-quality):</strong> includes Wikipedia, high-quality web, ~20% math, plus code/reasoning sources</li>
</ul>
</details>

<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="three-training-pipelines">Three training pipelines</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>Let <strong>M<sub>t</sub></strong> be the base checkpoint after <em>t</em> pretraining steps/tokens.</p>
<p>We compare:</p>
<ol>
<li><p><strong>RL only:</strong><br><strong>M<sub>t</sub> → M<sub>t</sub><sup>RL</sup></strong><br>Run RL directly on the base checkpoint.</p>
</li>
<li><p><strong>SFT only:</strong><br><strong>M<sub>t</sub> → M<sub>t</sub><sup>SFT</sup></strong><br>Train on ground-truth solutions (teacher-written reasoning traces).</p>
</li>
<li><p><strong>Gold-standard pipeline:</strong><br><strong>M<sub>t</sub> → M<sub>t</sub><sup>SFT</sup> → M<sub>t</sub><sup>SFT→RL</sup></strong><br>SFT then RL, which is the typical modern recipe.</p>
</li>
</ol>
<p>Here’s a diagram you can paste into any Mermaid-enabled renderer:</p>
<pre><code class="language-mermaid">flowchart LR
  Mt[&quot;M_t (pretraining checkpoint)&quot;] --&gt;|RLVR (GRPO)| MRL[&quot;M_t^RL (RL-only)&quot;]
  Mt --&gt;|SFT on solutions| MSFT[&quot;M_t^SFT (SFT-only)&quot;]
  MSFT --&gt;|RLVR (GRPO)| MSFTRL[&quot;M_t^(SFT→RL) (standard pipeline)&quot;]
</code></pre>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="rl-objective-rlvr-with-grpo">RL objective: RLVR with GRPO</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>We use <strong>Reinforcement Learning from Verifiable Rewards (RLVR)</strong> with <strong>GRPO</strong>: the model generates solutions and receives a reward based on whether the final answer is correct (plus a formatting reward so models learn to follow the expected answer format).</p>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="data-and-evaluation">Data and evaluation</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><ul>
<li><strong>Training data:</strong> OpenMathInstruct (math questions with multiple ground-truth solutions)<ul>
<li>We use either (a) a GSM8K-like subset or (b) the full MATH-heavy mix.</li>
</ul>
</li>
<li><strong>Benchmarks:</strong> GSM8K and MATH</li>
<li><strong>Metric:</strong> pass@k for k ∈ {1, 8, 32} at temperature T = 0.6<br>(probability that at least one of k samples is correct)</li>
</ul>
<details>
<summary><strong>Why pass@k? (and how to read it)</strong></summary>

<ul>
<li><strong>pass@1</strong> ≈ “How often the model’s first answer is correct.”</li>
<li><strong>pass@32</strong> ≈ “If you sample 32 times and take the best, how often do you get at least one correct answer?”</li>
</ul>
<p>pass@k is a nice lens because it separates:</p>
<ul>
<li>improvements due to better <em>typical</em> behavior (pass@1), and</li>
<li>improvements due to better <em>coverage</em> / diversity of correct reasoning paths (larger k).</li>
</ul>
</details>

<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="a-small-evaluation-gotcha-important">A small evaluation gotcha (important!)</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>Early pretraining checkpoints don’t reliably follow instruction formatting. To evaluate base checkpoints fairly:</p>
<ul>
<li><strong>Base checkpoints M<sub>t</sub></strong> are evaluated with <strong>8-shot</strong> prompting (few-shot examples teach the format).</li>
<li><strong>All trained models (SFT/RL variants)</strong> are evaluated <strong>0-shot</strong>, because they learn the format during training.</li>
</ul>
<hr>
<!-- HTML_TAG_END --></div></div> </details></div> </details><details class="foldbox foldbox--h2 svelte-1cybysc" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h2 id="result-1-rl-is-effective-early-in-pretraining">Result 1: RL is effective early in pretraining</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>Let’s start with the simplest question:</p>
<blockquote class="inline-block bg-neutral-50 border-l-4 border-neutral-600 rounded px-3 py-2 align-middle my-2"><p><strong>If you take an early checkpoint and run RL, does anything good happen?</strong></p>
</blockquote><!-- HTML_TAG_END --></div><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="gsm8k-yes-and-surprisingly-early">GSM8K: yes — and surprisingly early</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>On GSM8K-style training, RL on early checkpoints produces large gains.</p>
<figure>
  <img src="/rl-excursions-during-pretraining/assets/figures/gsm_passatk_comparison.png" alt="Placeholder for GSM8K pass@1, pass@8, pass@32 vs pretraining tokens for base, RL-only, SFT-only, and SFT→RL." width="100%"/>
  <figcaption><strong>Figure 2 (placeholder).</strong> GSM8K results across checkpoints. RL-only improves early and can match SFT→RL after enough pretraining.</figcaption>
</figure>

<p>One striking data point: <strong>at ~4B pretraining tokens</strong>, RL raises GSM8K <strong>pass@1</strong> from about <strong>2% → 18%</strong>.</p>
<p>Even more interesting: once the checkpoint is later (≈10B+ tokens), <strong>RL-only is competitive with the standard SFT→RL pipeline</strong>.</p>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="why-that-s-surprising">Why that’s surprising</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>RL-only never trains on ground-truth reasoning traces. It only sees:</p>
<ul>
<li>its <em>own</em> generated solutions, and</li>
<li>a reward for correctness (plus format).</li>
</ul>
<p>Yet it can match a pipeline that explicitly trains on solutions.</p>
<p>A practical implication is that <strong>ground-truth solution traces may not be strictly necessary</strong> to unlock certain reasoning behaviors, as long as you can supply a <em>verifiable</em> reward.</p>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="math-rl-helps-but-hits-a-ceiling">MATH: RL helps, but hits a ceiling</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>Now for the harder benchmark.</p>
<figure>
  <img src="/rl-excursions-during-pretraining/assets/figures/math_passatk_comparison.png" alt="Placeholder for MATH pass@k results vs pretraining tokens. RL-only improves but lags SFT and SFT→RL." width="100%"/>
  <figcaption><strong>Figure 3 (placeholder).</strong> MATH results. RL-only improves over the base checkpoint but doesn’t catch up to SFT or SFT→RL on this harder distribution.</figcaption>
</figure>

<p>On MATH-heavy training, RL still improves over the base model — but <strong>does not reach</strong> the performance of SFT or SFT→RL at later checkpoints.</p>
<p>One interpretation: <strong>on-policy learning from self-generated traces has limits when the task is too hard</strong>, because early samples contain too few correct trajectories to learn from.</p>
<p>That observation motivates the next two sections: what RL is doing to the output distribution, and how rollout budgets affect sparse-reward learning.</p>
<hr>
<!-- HTML_TAG_END --></div></div> </details></div> </details><details class="foldbox foldbox--h2 svelte-1cybysc" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h2 id="result-2-rl-can-expand-or-sharpen-the-output-distribution">Result 2: RL can expand or sharpen the output distribution</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>A recurring claim in recent RLVR work is:</p>
<blockquote class="inline-block bg-neutral-50 border-l-4 border-neutral-600 rounded px-3 py-2 align-middle my-2"><p>RL mostly <strong>sharpens</strong> — it improves pass@1, but doesn’t increase pass@k for large k.</p>
</blockquote><p>We test this claim and find: <strong>it depends.</strong></p>
<!-- HTML_TAG_END --></div><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="two-behaviors">Two behaviors</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>We’ll use these terms:</p>
<ul>
<li><p><strong>Sharpening:</strong> pass@1 improves, pass@k (large k) doesn’t improve (or even decreases).<br><em>Intuition:</em> the model concentrates probability mass on a smaller set of solutions.</p>
</li>
<li><p><strong>Expansion:</strong> pass@1 and pass@k both improve.<br><em>Intuition:</em> the model discovers more correct “modes” — new successful reasoning paths.</p>
</li>
</ul>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="standard-pipeline-sft-rl-tends-to-sharpen">Standard pipeline (SFT→RL) tends to sharpen</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>When RL comes <em>after</em> SFT, we often see:</p>
<ul>
<li>pass@1 goes up,</li>
<li>pass@32 goes down a bit.</li>
</ul>
<figure>
  <img src="/rl-excursions-during-pretraining/assets/figures/gsm8k_rl_train_dynamics_comparison.png" alt="Placeholder: training dynamics showing sharpening in SFT→RL and expansion in RL-only." width="100%"/>
  <figcaption><strong>Figure 4 (placeholder).</strong> Training dynamics. Left: SFT→RL shows sharpening (pass@1 up, pass@32 down during RL). Right: RL-only shows expansion (both pass@1 and pass@32 up).</figcaption>
</figure>

<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="rl-only-tends-to-expand-in-our-setting">RL-only tends to expand (in our setting)</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>When we run RL directly on the base checkpoint, we consistently see <strong>pass@32 improve</strong>, suggesting <strong>expansion</strong>.</p>
<p>A plausible explanation:</p>
<ul>
<li>After SFT, the model has already been shown ground-truth solutions for the same training questions.</li>
<li>RL then mainly “locks in” the highest-reward paths it already knows, reducing exploration/diversity (sharpening).</li>
<li>Without SFT, RL must explore more to find reward at all — and in doing so, it can discover new successful traces (expansion).</li>
</ul>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="a-caution-early-rl-can-be-brittle">A caution: early RL can be brittle</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>This is not all upside. RL-only on <em>very early</em> checkpoints is <strong>unstable across random seeds</strong>.</p>
<figure>
  <img src="/rl-excursions-during-pretraining/assets/figures/gsm8k_seed_rewards.png" alt="Placeholder: early checkpoint RL seed brittleness—training reward similar, but test pass@k diverges." width="100%"/>
  <figcaption><strong>Figure 7 (placeholder).</strong> Seed brittleness at early checkpoints: training reward can look similar while test performance diverges sharply.</figcaption>
</figure>

<p>What this suggests is subtle but important:</p>
<ul>
<li><strong>Training reward is not always a reliable proxy</strong> for “real” reasoning improvements when the base model is weak.</li>
<li>Early on-policy learning may latch onto superficial patterns that maximize reward on the training distribution without learning robust reasoning.</li>
</ul>
<hr>
<!-- HTML_TAG_END --></div></div> </details></div> </details><details class="foldbox foldbox--h2 svelte-1cybysc" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h2 id="result-3-rollout-budgets-under-sparse-rewards">Result 3: rollout budgets under sparse rewards</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>Early checkpoints suffer from a basic problem: <strong>reward sparsity</strong>.</p>
<p>If the model almost never produces a correct solution, then the RL signal becomes:</p>
<ul>
<li>sparse (few positives),</li>
<li>noisy (high-variance gradients),</li>
<li>and potentially misleading (format hacks, memorization, etc.).</li>
</ul>
<p>A natural knob in GRPO is <strong>n = number of rollouts per prompt</strong>.</p>
<blockquote class="inline-block bg-neutral-50 border-l-4 border-neutral-600 rounded px-3 py-2 align-middle my-2"><p>If correct solutions are rare, maybe sampling more rollouts per prompt makes learning possible?</p>
</blockquote><!-- HTML_TAG_END --></div><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="the-experiment">The experiment</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>We simulate “early” vs “late” competence by splitting the GSM8K-like training set into:</p>
<ul>
<li><strong>GSM8K-Easy:</strong> prompts where the base model already gets many correct samples (16–64 correct out of 64 rollouts).</li>
<li><strong>GSM8K-Hard:</strong> prompts where the base model gets few correct samples (≤8 correct out of 64 rollouts).</li>
</ul>
<p>Then we run RL with:</p>
<ul>
<li><strong>n = 5</strong> rollouts per prompt, vs</li>
<li><strong>n = 64</strong> rollouts per prompt,</li>
</ul>
<p>and measure GSM8K test pass@k as a function of:</p>
<ul>
<li><strong>samples seen</strong>, and</li>
<li><strong>FLOPs</strong>.</li>
</ul>
<figure>
  <img src="/rl-excursions-during-pretraining/assets/figures/gsm8k_rollouts_p1-2.png" alt="Placeholder: effect of rollouts on pass@1 and pass@8 vs FLOPs and vs samples." width="100%"/>
  <figcaption><strong>Figure 5 (placeholder).</strong> Rollout scaling trade-offs. More rollouts improves sample efficiency, but fewer rollouts can be more FLOP-efficient—especially on the hard split.</figcaption>
</figure>

<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="what-we-learned">What we learned</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>Three takeaways:</p>
<ol>
<li><strong>Asymptotic performance is similar.</strong> Both n=5 and n=64 often converge to similar best pass@k.</li>
<li><strong>n=64 is more sample-efficient.</strong> You learn faster per training example (per prompt).</li>
<li><strong>n=5 can be more FLOP-efficient.</strong> Especially early in training and on harder splits where reward is sparse, spending compute on <em>more prompts</em> (not more rollouts per prompt) wins.</li>
</ol>
<p>This lines up with an intuition from exploration: when rewards are rare, it can be better to <strong>see more states</strong> rather than spend many samples on the same state.</p>
<hr>
<!-- HTML_TAG_END --></div></div> </details></div> </details><details class="foldbox foldbox--h2 svelte-1cybysc" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h2 id="practical-takeaways">Practical takeaways</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>If you’re training reasoning models with RLVR today, here’s what this study suggests.</p>
<!-- HTML_TAG_END --></div><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="1-rl-readiness-comes-earlier-than-you-think-for-some-tasks">1) “RL readiness” comes earlier than you think (for some tasks)</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>For GSM8K-like math with verifiable rewards, on-policy learning starts paying off <em>very early</em> in pretraining. You don’t necessarily need to wait for a fully-finished base model.</p>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="2-track-pass-k-not-just-pass-1">2) Track pass@k, not just pass@1</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>If you only look at pass@1, you can miss whether RL is:</p>
<ul>
<li><strong>sharpening</strong> (maybe good for deterministic deployment), or</li>
<li><strong>expanding</strong> (good for best-of-k / tool-assisted settings).</li>
</ul>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="3-expect-brittleness-on-weak-checkpoints">3) Expect brittleness on weak checkpoints</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>If you try RL very early:</p>
<ul>
<li>run multiple seeds,</li>
<li>use validation metrics that correlate with generalization,</li>
<li>and watch for “reward without reasoning” failure modes.</li>
</ul>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="4-don-t-blindly-scale-rollouts-per-prompt">4) Don’t blindly scale rollouts per prompt</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>If your budget is fixed, <strong>n is a trade-off</strong>:</p>
<ul>
<li>increase n when you need <em>sample efficiency</em> (limited dataset, limited prompts),</li>
<li>decrease n when you care about <em>FLOP efficiency</em> (limited compute) or want broader coverage across prompts.</li>
</ul>
<hr>
<!-- HTML_TAG_END --></div></div> </details></div> </details><details class="foldbox foldbox--h2 svelte-1cybysc" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h2 id="limitations-and-open-questions">Limitations and open questions</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>This study is intentionally narrow: it’s a controlled probe of <em>when</em> RL can help, not a full replacement recipe for pretraining.</p>
<p>Some important limitations:</p>
<ul>
<li><strong>Task scope:</strong> math reasoning with verifiable rewards is unusually clean.</li>
<li><strong>Data mixture:</strong> our base model is pretrained on a corpus that includes a substantial fraction of math/reasoning content; “RL readiness” may shift with pretraining mix.</li>
<li><strong>Model scale:</strong> results are from a 1B model; larger models may show different transitions.</li>
<li><strong>Algorithm scope:</strong> we used RLVR with GRPO; other RL algorithms or denser rewards (e.g., process reward) could change the picture.</li>
</ul>
<p>Open directions we’re excited about:</p>
<ul>
<li><strong>Mixing RL into pretraining:</strong> can we interleave RL and next-token prediction in a stable way?</li>
<li><strong>Curricula for early RL:</strong> can we schedule task difficulty (or reward shaping) so early checkpoints don’t get stuck?</li>
<li><strong>Understanding expansion vs sharpening:</strong> when does RL discover new modes vs collapse onto existing ones?</li>
<li><strong>Generalization beyond math:</strong> what are the right “verifiable rewards” in other domains?</li>
</ul>
<hr>
<hr>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h2 svelte-1cybysc" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h2 id="appendix-additional-training-curves-and-ablations">Appendix: additional training curves and ablations</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>These plots are useful for readers who want to sanity-check training stability and evaluation choices.</p>
<details>
<summary><strong>RL training convergence across checkpoints (Figure 6)</strong></summary>

<figure>
  <img src="/rl-excursions-during-pretraining/assets/figures/gsm8k_rl_sft_comparison.png" alt="Placeholder: RL train/val reward and GSM8K pass@1 over RL steps for multiple pretraining checkpoints." width="100%"/>
  <figcaption><strong>Figure 6 (placeholder).</strong> RL reward curves (train/val) and GSM8K pass@1 over RL steps show convergence across checkpoints.</figcaption>
</figure>

</details>

<details>
<summary><strong>SFT convergence (Figure 8)</strong></summary>

<figure>
  <img src="/rl-excursions-during-pretraining/assets/figures/appx_fixB_easy.png" alt="Placeholder: SFT epoch comparison (5 vs 10 epochs) showing convergence across checkpoints on GSM8K pass@k." width="100%"/>
  <figcaption><strong>Figure 8 (placeholder).</strong> SFT epoch ablation indicates performance converges by ~5 epochs.</figcaption>
</figure>

</details>

<details>
<summary><strong>How we evaluate base checkpoints (Figure 9)</strong></summary>

<figure>
  <img src="/rl-excursions-during-pretraining/assets/figures/gsm8k_base_eval_shots.png" alt="Placeholder: n-shot prompting ablation (0/1/8-shot) for evaluating base checkpoints on GSM8K and MATH pass@k." width="100%"/>
  <figcaption><strong>Figure 9 (placeholder).</strong> Few-shot prompting ablation for base checkpoints: 8-shot yields the strongest evaluation performance.</figcaption>
</figure>

</details>

<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h2 svelte-1cybysc" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h2 id="citation">Citation</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>If you build on this work, please cite the accompanying paper:</p>
<pre><code class="language-bibtex">@article{anonymous2026rlexcursions,
  title   = {RL Excursions During Pre-Training: How Early Is Too Early for On-Policy Learning?},
  author  = {Anonymous Authors},
  journal = {Under review},
  year    = {2026}
}
</code></pre>
<hr>
<p><em>If you want to adapt this markdown for a project page (like a GitHub Pages site), you can:</em></p>
<ul>
<li>export your plots from the paper into <code>assets/</code>,</li>
<li>replace the placeholders,</li>
<li>and optionally add interactive plot embeds (Plotly/Observable) for Figures 2–5.</li>
</ul>
<!-- HTML_TAG_END --></div></div> </details></div> </div> </div></div></div></main>  
			
			<script>
				{
					__sveltekit_x3glat = {
						base: new URL("..", location).pathname.slice(0, -1),
						assets: "/rl-excursions-during-pretraining"
					};

					const element = document.currentScript.parentElement;

					const data = [null,null];

					Promise.all([
						import("../_app/immutable/entry/start.Dz3fLiJ5.js"),
						import("../_app/immutable/entry/app.nvD-W9l8.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 3],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
  </body>
</html>
