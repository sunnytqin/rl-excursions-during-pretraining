const textRaw = '<!-- ---\ntitle: "RL Excursions During Pre-Training: How Early Is Too Early for On-Policy Learning?"\nsubtitle: "When can a language model start learning from its *own* generations?"\nauthors:\n  - "Anonymous authors"\ndate: "2026-02-11"\ntags: [llm-training, reinforcement-learning, reasoning, grpo, rlvr, pretraining]\n--- -->\n\n<!--\nThis post is a blog-style adaptation of the paper:\n"RL Excursions During Pre-Training: How Early Is Too Early for On-Policy Learning?"\nReplace placeholder figure links in `assets/` with real images exported from the paper.\n-->\n\n<!-- # RL excursions during pre-training: how early is *too* early for on-policy learning? -->\n\n<figure>\n  <img src="/assets/figures/figure_1.png" alt="Overview figure placeholder showing early RL works, expansion vs sharpening, and rollout budget tradeoffs." width="100%"/>\n  <figcaption><strong>Figure 1 (placeholder).</strong> A one-picture summary: RL works surprisingly early; RL can <em>expand</em> or <em>sharpen</em> the output distribution depending on the pipeline; rollout count trades off sample-efficiency vs FLOP-efficiency.</figcaption>\n</figure>\n\nModern LLM training usually looks like this:\n\n> **Pretrain (next-token prediction)** → **SFT (next-token prediction)** → **RL (on-policy)**\n\nThis separation raises a simple question that we don’t often test directly:\n\n**When does a model become capable of learning from its own generations?**\n\nIn this post we run a controlled case study on math reasoning, where rewards are unambiguous, and ask:\n\n> **How and when should an RL objective be used in LLM training?**\n\n---\n\n## TL;DR\n\n- **RL can work *much earlier* than standard practice.** On GSM8K-style math, running RL directly on intermediate pretraining checkpoints boosts accuracy even when the model is still “under-trained” by standard scaling-law heuristics.\n- **RL-only can match the “gold standard” SFT→RL pipeline (sometimes).** On GSM8K, direct RL on the base checkpoint reaches performance comparable to SFT→RL once the model has seen enough pretraining tokens.\n- **RL doesn’t always just “sharpen.”** In our setting, RL directly on the base model tends to improve both pass@1 *and* pass@k (distribution **expansion**), while RL after SFT often improves pass@1 but hurts pass@k (distribution **sharpening**).\n- **More rollouts aren’t always better.** Increasing GRPO rollouts per prompt improves *sample efficiency* but can be *worse per FLOP*, especially when rewards are sparse.\n\nIf you only remember one thing: **“Post-training” might not need to wait until *after* pretraining.**\n\n---\n\n## Table of contents\n\n- [The puzzle: off-policy vs on-policy training](#the-puzzle-off-policy-vs-on-policy-training)\n- [Experimental setup in one page](#experimental-setup-in-one-page)\n- [Result 1: RL is effective early in pretraining](#result-1-rl-is-effective-early-in-pretraining)\n- [Result 2: RL can expand or sharpen the output distribution](#result-2-rl-can-expand-or-sharpen-the-output-distribution)\n- [Result 3: rollout budgets under sparse rewards](#result-3-rollout-budgets-under-sparse-rewards)\n- [Practical takeaways](#practical-takeaways)\n- [Limitations and open questions](#limitations-and-open-questions)\n- [Citation](#citation)\n- [Appendix: additional training curves and ablations](#appendix-additional-training-curves-and-ablations)\n\n---\n\n## The puzzle: off-policy vs on-policy training\n\nPretraining and supervised fine-tuning (SFT) optimize the **next-token prediction** objective. Conceptually, that’s an *off-policy* regime: the model is trained to imitate a fixed dataset.\n\nReinforcement learning (RL), in contrast, is **on-policy**: the model samples outputs from itself, gets feedback, and updates to increase reward.\n\nSo the transition from SFT to RL is a big change:\n\n- from *imitation* → to *self-generated data*\n- from *dense token-level supervision* → to *sparse outcome-level supervision* (at least in the RLVR setting we study)\n\nThat makes the following question natural:\n\n> **At what point during pretraining does the model’s self-generated data become good enough that on-policy learning helps rather than hurts?**\n\nMath is a clean testbed because the reward can be **verifiable** (did the final answer match?).\n\n---\n\n## Experimental setup in one page\n\n### What we trained\n\nWe pretrain a **1B-parameter** decoder-only model (OLMo2 architecture) from scratch on **50B tokens** of a high-quality mixture (DOLMino, from OLMo2), saving intermediate checkpoints throughout. We then take checkpoints and run different “post-training” pipelines *from each checkpoint*.\n\n<details>\n<summary><strong>Pretraining details (click to expand)</strong></summary>\n\n- **Architecture:** OLMo2 1B\n- **Tokens:** 50B total (≈ 2.5× Chinchilla-optimal token count for this model size)\n- **Optimizer:** AdamW with cosine LR decay, peak LR 4e-4\n- **Seq length:** 4096\n- **Batch size:** 512\n- **Data mixture (DOLMino high-quality):** includes Wikipedia, high-quality web, ~20% math, plus code/reasoning sources\n\n</details>\n\n### Three training pipelines\n\nLet **M<sub>t</sub>** be the base checkpoint after *t* pretraining steps/tokens.\n\nWe compare:\n\n1. **RL only:**  \n   **M<sub>t</sub> → M<sub>t</sub><sup>RL</sup>**  \n   Run RL directly on the base checkpoint.\n\n2. **SFT only:**  \n   **M<sub>t</sub> → M<sub>t</sub><sup>SFT</sup>**  \n   Train on ground-truth solutions (teacher-written reasoning traces).\n\n3. **Gold-standard pipeline:**  \n   **M<sub>t</sub> → M<sub>t</sub><sup>SFT</sup> → M<sub>t</sub><sup>SFT→RL</sup>**  \n   SFT then RL, which is the typical modern recipe.\n\nHere’s a diagram you can paste into any Mermaid-enabled renderer:\n\n```mermaid\nflowchart LR\n  Mt["M_t (pretraining checkpoint)"] -->|RLVR (GRPO)| MRL["M_t^RL (RL-only)"]\n  Mt -->|SFT on solutions| MSFT["M_t^SFT (SFT-only)"]\n  MSFT -->|RLVR (GRPO)| MSFTRL["M_t^(SFT→RL) (standard pipeline)"]\n```\n\n### RL objective: RLVR with GRPO\n\nWe use **Reinforcement Learning from Verifiable Rewards (RLVR)** with **GRPO**: the model generates solutions and receives a reward based on whether the final answer is correct (plus a formatting reward so models learn to follow the expected answer format).\n\n### Data and evaluation\n\n- **Training data:** OpenMathInstruct (math questions with multiple ground-truth solutions)\n  - We use either (a) a GSM8K-like subset or (b) the full MATH-heavy mix.\n- **Benchmarks:** GSM8K and MATH\n- **Metric:** pass@k for k ∈ {1, 8, 32} at temperature T = 0.6  \n  (probability that at least one of k samples is correct)\n\n<details>\n<summary><strong>Why pass@k? (and how to read it)</strong></summary>\n\n- **pass@1** ≈ “How often the model’s first answer is correct.”\n- **pass@32** ≈ “If you sample 32 times and take the best, how often do you get at least one correct answer?”\n\npass@k is a nice lens because it separates:\n- improvements due to better *typical* behavior (pass@1), and\n- improvements due to better *coverage* / diversity of correct reasoning paths (larger k).\n\n</details>\n\n### A small evaluation gotcha (important!)\n\nEarly pretraining checkpoints don’t reliably follow instruction formatting. To evaluate base checkpoints fairly:\n\n- **Base checkpoints M<sub>t</sub>** are evaluated with **8-shot** prompting (few-shot examples teach the format).\n- **All trained models (SFT/RL variants)** are evaluated **0-shot**, because they learn the format during training.\n\n---\n\n## Result 1: RL is effective early in pretraining\n\nLet’s start with the simplest question:\n\n> **If you take an early checkpoint and run RL, does anything good happen?**\n\n### GSM8K: yes — and surprisingly early\n\nOn GSM8K-style training, RL on early checkpoints produces large gains.\n\n<figure>\n  <img src="/assets/figures/gsm_passatk_comparison.png" alt="Placeholder for GSM8K pass@1, pass@8, pass@32 vs pretraining tokens for base, RL-only, SFT-only, and SFT→RL." width="100%"/>\n  <figcaption><strong>Figure 2 (placeholder).</strong> GSM8K results across checkpoints. RL-only improves early and can match SFT→RL after enough pretraining.</figcaption>\n</figure>\n\nOne striking data point: **at ~4B pretraining tokens**, RL raises GSM8K **pass@1** from about **2% → 18%**.\n\nEven more interesting: once the checkpoint is later (≈10B+ tokens), **RL-only is competitive with the standard SFT→RL pipeline**.\n\n### Why that’s surprising\n\nRL-only never trains on ground-truth reasoning traces. It only sees:\n- its *own* generated solutions, and\n- a reward for correctness (plus format).\n\nYet it can match a pipeline that explicitly trains on solutions.\n\nA practical implication is that **ground-truth solution traces may not be strictly necessary** to unlock certain reasoning behaviors, as long as you can supply a *verifiable* reward.\n\n### MATH: RL helps, but hits a ceiling\n\nNow for the harder benchmark.\n\n<figure>\n  <img src="/assets/figures/math_passatk_comparison.png" alt="Placeholder for MATH pass@k results vs pretraining tokens. RL-only improves but lags SFT and SFT→RL." width="100%"/>\n  <figcaption><strong>Figure 3 (placeholder).</strong> MATH results. RL-only improves over the base checkpoint but doesn’t catch up to SFT or SFT→RL on this harder distribution.</figcaption>\n</figure>\n\nOn MATH-heavy training, RL still improves over the base model — but **does not reach** the performance of SFT or SFT→RL at later checkpoints.\n\nOne interpretation: **on-policy learning from self-generated traces has limits when the task is too hard**, because early samples contain too few correct trajectories to learn from.\n\nThat observation motivates the next two sections: what RL is doing to the output distribution, and how rollout budgets affect sparse-reward learning.\n\n---\n\n## Result 2: RL can expand or sharpen the output distribution\n\nA recurring claim in recent RLVR work is:\n\n> RL mostly **sharpens** — it improves pass@1, but doesn’t increase pass@k for large k.\n\nWe test this claim and find: **it depends.**\n\n### Two behaviors\n\nWe’ll use these terms:\n\n- **Sharpening:** pass@1 improves, pass@k (large k) doesn’t improve (or even decreases).  \n  *Intuition:* the model concentrates probability mass on a smaller set of solutions.\n\n- **Expansion:** pass@1 and pass@k both improve.  \n  *Intuition:* the model discovers more correct “modes” — new successful reasoning paths.\n\n### Standard pipeline (SFT→RL) tends to sharpen\n\nWhen RL comes *after* SFT, we often see:\n\n- pass@1 goes up,\n- pass@32 goes down a bit.\n\n<figure>\n  <img src="/assets/figures/gsm8k_rl_train_dynamics_comparison.png" alt="Placeholder: training dynamics showing sharpening in SFT→RL and expansion in RL-only." width="100%"/>\n  <figcaption><strong>Figure 4 (placeholder).</strong> Training dynamics. Left: SFT→RL shows sharpening (pass@1 up, pass@32 down during RL). Right: RL-only shows expansion (both pass@1 and pass@32 up).</figcaption>\n</figure>\n\n### RL-only tends to expand (in our setting)\n\nWhen we run RL directly on the base checkpoint, we consistently see **pass@32 improve**, suggesting **expansion**.\n\nA plausible explanation:\n\n- After SFT, the model has already been shown ground-truth solutions for the same training questions.\n- RL then mainly “locks in” the highest-reward paths it already knows, reducing exploration/diversity (sharpening).\n- Without SFT, RL must explore more to find reward at all — and in doing so, it can discover new successful traces (expansion).\n\n### A caution: early RL can be brittle\n\nThis is not all upside. RL-only on *very early* checkpoints is **unstable across random seeds**.\n\n<figure>\n  <img src="/assets/figures/gsm8k_seed_rewards.png" alt="Placeholder: early checkpoint RL seed brittleness—training reward similar, but test pass@k diverges." width="100%"/>\n  <figcaption><strong>Figure 7 (placeholder).</strong> Seed brittleness at early checkpoints: training reward can look similar while test performance diverges sharply.</figcaption>\n</figure>\n\nWhat this suggests is subtle but important:\n\n- **Training reward is not always a reliable proxy** for “real” reasoning improvements when the base model is weak.\n- Early on-policy learning may latch onto superficial patterns that maximize reward on the training distribution without learning robust reasoning.\n\n---\n\n## Result 3: rollout budgets under sparse rewards\n\nEarly checkpoints suffer from a basic problem: **reward sparsity**.\n\nIf the model almost never produces a correct solution, then the RL signal becomes:\n- sparse (few positives),\n- noisy (high-variance gradients),\n- and potentially misleading (format hacks, memorization, etc.).\n\nA natural knob in GRPO is **n = number of rollouts per prompt**.\n\n> If correct solutions are rare, maybe sampling more rollouts per prompt makes learning possible?\n\n### The experiment\n\nWe simulate “early” vs “late” competence by splitting the GSM8K-like training set into:\n\n- **GSM8K-Easy:** prompts where the base model already gets many correct samples (16–64 correct out of 64 rollouts).\n- **GSM8K-Hard:** prompts where the base model gets few correct samples (≤8 correct out of 64 rollouts).\n\nThen we run RL with:\n- **n = 5** rollouts per prompt, vs\n- **n = 64** rollouts per prompt,\n\nand measure GSM8K test pass@k as a function of:\n- **samples seen**, and\n- **FLOPs**.\n\n<figure>\n  <img src="/assets/figures/gsm8k_rollouts_p1-2.png" alt="Placeholder: effect of rollouts on pass@1 and pass@8 vs FLOPs and vs samples." width="100%"/>\n  <figcaption><strong>Figure 5 (placeholder).</strong> Rollout scaling trade-offs. More rollouts improves sample efficiency, but fewer rollouts can be more FLOP-efficient—especially on the hard split.</figcaption>\n</figure>\n\n### What we learned\n\nThree takeaways:\n\n1. **Asymptotic performance is similar.** Both n=5 and n=64 often converge to similar best pass@k.\n2. **n=64 is more sample-efficient.** You learn faster per training example (per prompt).\n3. **n=5 can be more FLOP-efficient.** Especially early in training and on harder splits where reward is sparse, spending compute on *more prompts* (not more rollouts per prompt) wins.\n\nThis lines up with an intuition from exploration: when rewards are rare, it can be better to **see more states** rather than spend many samples on the same state.\n\n---\n\n## Practical takeaways\n\nIf you’re training reasoning models with RLVR today, here’s what this study suggests.\n\n### 1) “RL readiness” comes earlier than you think (for some tasks)\n\nFor GSM8K-like math with verifiable rewards, on-policy learning starts paying off *very early* in pretraining. You don’t necessarily need to wait for a fully-finished base model.\n\n### 2) Track pass@k, not just pass@1\n\nIf you only look at pass@1, you can miss whether RL is:\n\n- **sharpening** (maybe good for deterministic deployment), or\n- **expanding** (good for best-of-k / tool-assisted settings).\n\n### 3) Expect brittleness on weak checkpoints\n\nIf you try RL very early:\n- run multiple seeds,\n- use validation metrics that correlate with generalization,\n- and watch for “reward without reasoning” failure modes.\n\n### 4) Don’t blindly scale rollouts per prompt\n\nIf your budget is fixed, **n is a trade-off**:\n\n- increase n when you need *sample efficiency* (limited dataset, limited prompts),\n- decrease n when you care about *FLOP efficiency* (limited compute) or want broader coverage across prompts.\n\n---\n\n## Limitations and open questions\n\nThis study is intentionally narrow: it’s a controlled probe of *when* RL can help, not a full replacement recipe for pretraining.\n\nSome important limitations:\n\n- **Task scope:** math reasoning with verifiable rewards is unusually clean.\n- **Data mixture:** our base model is pretrained on a corpus that includes a substantial fraction of math/reasoning content; “RL readiness” may shift with pretraining mix.\n- **Model scale:** results are from a 1B model; larger models may show different transitions.\n- **Algorithm scope:** we used RLVR with GRPO; other RL algorithms or denser rewards (e.g., process reward) could change the picture.\n\nOpen directions we’re excited about:\n\n- **Mixing RL into pretraining:** can we interleave RL and next-token prediction in a stable way?\n- **Curricula for early RL:** can we schedule task difficulty (or reward shaping) so early checkpoints don’t get stuck?\n- **Understanding expansion vs sharpening:** when does RL discover new modes vs collapse onto existing ones?\n- **Generalization beyond math:** what are the right “verifiable rewards” in other domains?\n\n---\n\n\n\n---\n\n## Appendix: additional training curves and ablations\n\nThese plots are useful for readers who want to sanity-check training stability and evaluation choices.\n\n<details>\n<summary><strong>RL training convergence across checkpoints (Figure 6)</strong></summary>\n\n<figure>\n  <img src="/assets/figures/gsm8k_rl_sft_comparison.png" alt="Placeholder: RL train/val reward and GSM8K pass@1 over RL steps for multiple pretraining checkpoints." width="100%"/>\n  <figcaption><strong>Figure 6 (placeholder).</strong> RL reward curves (train/val) and GSM8K pass@1 over RL steps show convergence across checkpoints.</figcaption>\n</figure>\n\n</details>\n\n<details>\n<summary><strong>SFT convergence (Figure 8)</strong></summary>\n\n<figure>\n  <img src="/assets/figures/appx_fixB_easy.png" alt="Placeholder: SFT epoch comparison (5 vs 10 epochs) showing convergence across checkpoints on GSM8K pass@k." width="100%"/>\n  <figcaption><strong>Figure 8 (placeholder).</strong> SFT epoch ablation indicates performance converges by ~5 epochs.</figcaption>\n</figure>\n\n</details>\n\n<details>\n<summary><strong>How we evaluate base checkpoints (Figure 9)</strong></summary>\n\n<figure>\n  <img src="/assets/figures/gsm8k_base_eval_shots.png" alt="Placeholder: n-shot prompting ablation (0/1/8-shot) for evaluating base checkpoints on GSM8K and MATH pass@k." width="100%"/>\n  <figcaption><strong>Figure 9 (placeholder).</strong> Few-shot prompting ablation for base checkpoints: 8-shot yields the strongest evaluation performance.</figcaption>\n</figure>\n\n</details>\n\n## Citation\n\nIf you build on this work, please cite the accompanying paper:\n\n```bibtex\n@article{anonymous2026rlexcursions,\n  title   = {RL Excursions During Pre-Training: How Early Is Too Early for On-Policy Learning?},\n  author  = {Anonymous Authors},\n  journal = {Under review},\n  year    = {2026}\n}\n```\n\n---\n\n*If you want to adapt this markdown for a project page (like a GitHub Pages site), you can:*\n- export your plots from the paper into `assets/`,\n- replace the placeholders,\n- and optionally add interactive plot embeds (Plotly/Observable) for Figures 2–5.\n';
export {
  textRaw as t
};
